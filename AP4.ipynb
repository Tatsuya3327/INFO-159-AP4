{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eb62902-79dc-411e-a86c-f33ee50a07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import operator\n",
    "import nltk\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6124bff8-7494-49a6-9f5b-1cf56634f406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/t.k/opt/anaconda3/lib/python3.8/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to /Users/t.k/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84592fe0-ca39-40b4-adbc-35f72ad07e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            idd = cols[0]\n",
    "            label = cols[2].lstrip().rstrip()\n",
    "            text = cols[3]\n",
    "\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a732063-88ce-400a-a683-6e184d83abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def __init__(self, feature_method, trainX, trainY, devX, devY, testX, testY):\n",
    "        self.feature_vocab = {}\n",
    "        self.feature_method = feature_method\n",
    "        self.min_feature_count=2\n",
    "        self.log_reg = None\n",
    "\n",
    "        self.trainY=trainY\n",
    "        self.devY=devY\n",
    "        self.testY=testY\n",
    "        \n",
    "        self.trainX = self.process(trainX, training=True)\n",
    "        self.devX = self.process(devX, training=False)\n",
    "        self.testX = self.process(testX, training=False)\n",
    "\n",
    "    # Featurize entire dataset\n",
    "    def featurize(self, data):\n",
    "        featurized_data = []\n",
    "        for text in data:\n",
    "            feats = self.feature_method(text)\n",
    "            featurized_data.append(feats)\n",
    "        return featurized_data\n",
    "\n",
    "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
    "    def process(self, X_data, training = False):\n",
    "        \n",
    "        data = self.featurize(X_data)\n",
    "\n",
    "        if training:\n",
    "            fid = 0\n",
    "            feature_doc_count = Counter()\n",
    "            for feats in data:\n",
    "                for feat in feats:\n",
    "                    feature_doc_count[feat]+= 1\n",
    "\n",
    "            for feat in feature_doc_count:\n",
    "                if feature_doc_count[feat] >= self.min_feature_count:\n",
    "                    self.feature_vocab[feat] = fid\n",
    "                    fid += 1\n",
    "\n",
    "        F = len(self.feature_vocab)\n",
    "        D = len(data)\n",
    "        X = sparse.dok_matrix((D, F))\n",
    "        for idx, feats in enumerate(data):\n",
    "            for feat in feats:\n",
    "                if feat in self.feature_vocab:\n",
    "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "    # Train model and evaluate on held-out data\n",
    "    def train(self):\n",
    "        (D,F) = self.trainX.shape\n",
    "        best_dev_accuracy=0\n",
    "        best_model=None\n",
    "        for C in [0.1, 1, 10, 100]:\n",
    "            self.log_reg = linear_model.LogisticRegression(C = C, class_weight='balanced', max_iter=1000)\n",
    "            self.log_reg.fit(self.trainX, self.trainY)\n",
    "            training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
    "            development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
    "            if development_accuracy > best_dev_accuracy:\n",
    "                best_dev_accuracy=development_accuracy\n",
    "                best_model=self.log_reg\n",
    "\n",
    "            print(\"C: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (C, training_accuracy, development_accuracy))\n",
    "            \n",
    "        self.log_reg=best_model\n",
    "        \n",
    "        # save prediction\n",
    "        self.train_pred = self.log_reg.predict(self.trainX)\n",
    "        self.dev_pred = self.log_reg.predict(self.devX)\n",
    "        \n",
    "\n",
    "    def test(self):\n",
    "        \n",
    "        # save prediction\n",
    "        self.test_pred = self.log_reg.predict(self.testX)\n",
    "        \n",
    "        return self.log_reg.score(self.testX, self.testY)\n",
    "        \n",
    "\n",
    "    def printWeights(self, n=10):\n",
    "\n",
    "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
    "        for k in self.feature_vocab:\n",
    "            reverse_vocab[self.feature_vocab[k]]=k\n",
    "\n",
    "        # binary\n",
    "        if len(self.log_reg.classes_) == 2:\n",
    "              weights=self.log_reg.coef_[0]\n",
    "\n",
    "              cat=self.log_reg.classes_[1]\n",
    "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
    "              print()\n",
    "\n",
    "              cat=self.log_reg.classes_[0]\n",
    "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
    "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
    "              print()\n",
    "\n",
    "        # multiclass\n",
    "        else:\n",
    "          for i, cat in enumerate(self.log_reg.classes_):\n",
    "\n",
    "              weights=self.log_reg.coef_[i]\n",
    "\n",
    "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
    "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
    "              print()\n",
    "\n",
    "    # Plot confusion matrix on test data\n",
    "    def plot_confusion_matrix(self):\n",
    "        cm = confusion_matrix(self.testY, self.test_pred, normalize=\"all\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.log_reg.classes_)\n",
    "        disp.plot()\n",
    "        \n",
    "    # Print random samples of TN, FP, FN, TP\n",
    "    def print_confusion_matrix_examples(self, n=5):\n",
    "        print(\"Implementing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6cc973f-f55e-4d9b-877c-e0e3dddd0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_intervals(accuracy, n, significance_level):\n",
    "    critical_value=(1-significance_level)/2\n",
    "    z_alpha=-1*norm.ppf(critical_value)\n",
    "    se=math.sqrt((accuracy*(1-accuracy))/n)\n",
    "    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b7762c-b804-4101-a1c6-06292a2c8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(trainingFile, devFile, testFile, feature_method):\n",
    "    trainX, trainY=load_data(trainingFile)\n",
    "    devX, devY=load_data(devFile)\n",
    "    testX, testY=load_data(testFile)\n",
    "    \n",
    "    simple_classifier = Classifier(feature_method, trainX, trainY, devX, devY, testX, testY)\n",
    "    simple_classifier.train()\n",
    "    accuracy=simple_classifier.test()\n",
    "    \n",
    "    lower, upper=confidence_intervals(accuracy, len(testY), .95)\n",
    "    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n",
    "\n",
    "    simple_classifier.printWeights()\n",
    "    \n",
    "    return simple_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a7fb92-b871-4e6b-8809-fa5de0032c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_bow_featurize(text):\n",
    "    feats = {}\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        feats[word]=1\n",
    "            \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ca929-4c40-4171-82eb-5eb0da114cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1, Train accuracy: 0.960, Dev accuracy: 0.790\n",
      "C: 1, Train accuracy: 1.000, Dev accuracy: 0.720\n",
      "C: 10, Train accuracy: 1.000, Dev accuracy: 0.730\n",
      "C: 100, Train accuracy: 1.000, Dev accuracy: 0.720\n",
      "Test accuracy for best dev model: 0.750, 95% CIs: [0.665 0.835]\n",
      "\n",
      "pos\t0.454\t3\n",
      "pos\t0.313\t30\n",
      "pos\t0.301\t4\n",
      "pos\t0.270\tit\n",
      "pos\t0.266\tglass\n",
      "pos\t0.257\tcut\n",
      "pos\t0.251\tpreheat\n",
      "pos\t0.250\taround\n",
      "pos\t0.249\tif\n",
      "pos\t0.248\twith\n",
      "\n",
      "neg\t-0.285\tseason\n",
      "neg\t-0.262\twarm\n",
      "neg\t-0.261\tpowder\n",
      "neg\t-0.242\tvinegar\n",
      "neg\t-0.221\tsoda\n",
      "neg\t-0.216\tmore\n",
      "neg\t-0.206\tgarlic\n",
      "neg\t-0.201\thour\n",
      "neg\t-0.191\tparmesan\n",
      "neg\t-0.187\tbell\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingFile = \"train.txt\"\n",
    "devFile = \"dev.txt\"\n",
    "testFile = \"test.txt\"\n",
    "    \n",
    "model = run(trainingFile, devFile, testFile, binary_bow_featurize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24befb1b-ebdd-41a1-833f-95b98d64cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "# 1. lower\n",
    "# 2. remove stop words\n",
    "# 3. convert number to same value\n",
    "\n",
    "def custom_featurize(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    feats = {}\n",
    "    \n",
    "    # word_length\n",
    "    feats['num_tokens'] = len(text.split())\n",
    "    \n",
    "    # tokenize\n",
    "    words = nltk.word_tokenize(text)\n",
    "            \n",
    "    for word in words:\n",
    "        # remove stopwords\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # convert to lowercase\n",
    "        word = word.lower()\n",
    "\n",
    "        # convert numbers to 'num_symbol'\n",
    "        word = 'num_symbol' if word.isnumeric() else word\n",
    "            \n",
    "        feats[word] = 1\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc132dca-37f0-46e5-bffa-1683a95509a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1, Train accuracy: 0.933, Dev accuracy: 0.760\n",
      "C: 1, Train accuracy: 1.000, Dev accuracy: 0.780\n",
      "C: 10, Train accuracy: 1.000, Dev accuracy: 0.770\n",
      "C: 100, Train accuracy: 1.000, Dev accuracy: 0.770\n",
      "Test accuracy for best dev model: 0.790, 95% CIs: [0.710 0.870]\n",
      "\n",
      "pos\t0.603\taround\n",
      "pos\t0.591\ttablespoons\n",
      "pos\t0.587\tendive\n",
      "pos\t0.579\ttoast\n",
      "pos\t0.575\t3/4\n",
      "pos\t0.556\tmake\n",
      "pos\t0.550\tbroth\n",
      "pos\t0.541\tmustard\n",
      "pos\t0.523\tstill\n",
      "pos\t0.518\tglass\n",
      "\n",
      "neg\t-0.808\tseason\n",
      "neg\t-0.708\twarm\n",
      "neg\t-0.679\tsoda\n",
      "neg\t-0.671\tpowder\n",
      "neg\t-0.634\thour\n",
      "neg\t-0.596\tparmesan\n",
      "neg\t-0.557\tgarlic\n",
      "neg\t-0.553\t450°f\n",
      "neg\t-0.549\tbell\n",
      "neg\t-0.540\toften\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_preprocessed = run(trainingFile, devFile, testFile, custom_featurize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e905fb-76a1-4fb3-b424-96536b9d6121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
